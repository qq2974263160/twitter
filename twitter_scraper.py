import os
import time
import json
import logging
import tweepy
import urllib3
import ssl
from dotenv import load_dotenv
from datetime import datetime, timedelta
from requests.exceptions import SSLError
from urllib3.exceptions import SSLError as URLLibSSLError
import logging.handlers
from tqdm import tqdm
import colorama
from colorama import Fore, Style

# åŠ è½½é…ç½®
def load_config():
    """
    åŠ è½½JSONæ ¼å¼çš„é…ç½®æ–‡ä»¶
    è¿”å›: é…ç½®å­—å…¸
    å¼‚å¸¸: æ–‡ä»¶ä¸å­˜åœ¨ã€JSONæ ¼å¼é”™è¯¯ç­‰
    """
    try:
        with open('config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error("é…ç½®æ–‡ä»¶ config.json ä¸å­˜åœ¨")
        raise
    except json.JSONDecodeError:
        logger.error("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯")
        raise
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise

# é…ç½®æ—¥å¿—
def setup_logging():
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
    1. ä¸»æ—¥å¿—æ–‡ä»¶(twitter_scraper.log)
    2. é”™è¯¯æ—¥å¿—æ–‡ä»¶(twitter_scraper.error.log)
    3. æ€§èƒ½æ—¥å¿—æ–‡ä»¶(twitter_scraper.perf.log)
    4. æ§åˆ¶å°è¾“å‡º
    è¿”å›: æ ¹æ—¥å¿—è®°å½•å™¨
    """
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨(å¸¦æ—¥å¿—è½®è½¬)
    file_handler = logging.handlers.RotatingFileHandler(
        'twitter_scraper.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)

    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)

    # åˆ›å»ºé”™è¯¯æ—¥å¿—å¤„ç†å™¨
    error_handler = logging.handlers.RotatingFileHandler(
        'twitter_scraper.error.log',
        maxBytes=5*1024*1024,  # 5MB
        backupCount=3,
        encoding='utf-8'
    )
    error_handler.setFormatter(formatter)
    error_handler.setLevel(logging.ERROR)

    # è·å–æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    root_logger.handlers = []

    # æ·»åŠ å¤„ç†å™¨
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(error_handler)

    # åˆ›å»ºæ€§èƒ½æ—¥å¿—è®°å½•å™¨
    perf_logger = logging.getLogger('performance')
    perf_handler = logging.handlers.RotatingFileHandler(
        'twitter_scraper.perf.log',
        maxBytes=5*1024*1024,
        backupCount=2,
        encoding='utf-8'
    )
    perf_handler.setFormatter(formatter)
    perf_logger.addHandler(perf_handler)
    perf_logger.setLevel(logging.INFO)
    perf_logger.propagate = False  # ä¸ä¼ æ’­åˆ°çˆ¶è®°å½•å™¨

    return root_logger

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
logger = setup_logging()
perf_logger = logging.getLogger('performance')

# åŠ è½½ç¯å¢ƒå˜é‡å’Œé…ç½®
load_dotenv()
CONFIG = load_config()

# ä»é…ç½®æ–‡ä»¶è·å–å¸¸é‡
CACHE_FILE = CONFIG['cache_file']                # ç¼“å­˜æ–‡ä»¶è·¯å¾„
CACHE_EXPIRE_HOURS = CONFIG['cache_expire_hours'] # ç¼“å­˜è¿‡æœŸæ—¶é—´(å°æ—¶)
MAX_CACHE_SIZE = CONFIG['max_cache_size']        # æœ€å¤§ç¼“å­˜å¤§å°
MIN_RESULTS_PER_REQUEST = CONFIG['min_results_per_request']  # æ¯æ¬¡è¯·æ±‚æœ€å°‘ç»“æœæ•°
MAX_RESULTS_PER_REQUEST = 100   # Twitter APIæ”¯æŒæœ€å¤§100æ¡
RATE_LIMIT_WINDOW = CONFIG['rate_limit_window']  # é€Ÿç‡é™åˆ¶çª—å£(ç§’)
TWITTER_ACCOUNTS = CONFIG['twitter_accounts']     # Twitterè´¦å·é…ç½®

# Twitter APIè®¤è¯ä¿¡æ¯é…ç½®
TWITTER_ACCOUNTS = [
    {
        'name': 'account1',
        'bearer_token': 'AAAAAAAAAAAAAAAAAAAAAGo40AEAAAAArfHfelHEmRLeuog82WT4l8Fo7i8%3DaxDQ1xGblrstL9nN1ziJjqpIOOD0R0XeS8GixJjwSkMnaHwdnE',
        'api_key': '2bcxapYbbGz6aAjk62vfce6wX',
        'api_key_secret': 'lJGQcqk00x5fAjH6TDuR98BDs04HKjVp5brp543LQ4lSD85ELf',
        'access_token': '1902562784367595520-Y4AA4LtXzT6vPYBak02R9j7OkfFUt0',
        'access_token_secret': 'DJbWKGT1uItsFa7tNpkBaBNwnGR5HCWksBEvdARgU6PHZ'
    },
    {
        'name': 'account2',
        'bearer_token': 'AAAAAAAAAAAAAAAAAAAAAG890AEAAAAAbZe%2B4KRyx2GzE%2FJ4W%2BWgLQL9CVc%3D37DJbvfLfpSIUIZBSUnMFCieQIGpt34zxyOlrRKsMD49DX3fwY',
        'api_key': 'NTfK0nzyURltHIZi9hhbOyYdF',
        'api_key_secret': '3uz43PzgjvHJqCrbpPlwJsJgcbI6Cy9a5sQfzPjKWDqo0mxSiy',
        'access_token': '1836805399472947200-J8hQe0KCT9T0dF20wSCmMTjA0Yq1ng',
        'access_token_secret': 'hNdBu6AtRzKgQ4WwI2O9nfmoDqemVGdHbh1hNWIDwmzFW'
    }
]

# å…¨å±€é…ç½®å¸¸é‡
CACHE_FILE = 'twitter_cache.json'           # ç¼“å­˜æ–‡ä»¶å
CACHE_EXPIRE_HOURS = 24                     # ç¼“å­˜è¿‡æœŸæ—¶é—´
MAX_CACHE_SIZE = 100 * 1024 * 1024         # æœ€å¤§ç¼“å­˜å¤§å°(100MB)
MIN_RESULTS_PER_REQUEST = 5                 # å•æ¬¡è¯·æ±‚æœ€å°‘ç»“æœæ•°
MAX_RESULTS_PER_REQUEST = 10                # å•æ¬¡è¯·æ±‚æœ€å¤§ç»“æœæ•°
RATE_LIMIT_WINDOW = 900                     # é€Ÿç‡é™åˆ¶çª—å£(15åˆ†é’Ÿ)
EXPORT_FORMATS = ['json', 'csv']            # æ”¯æŒçš„å¯¼å‡ºæ ¼å¼

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# åˆå§‹åŒ–colorama
colorama.init()

class ProgressBar:
    """
    è¿›åº¦æ¡ç±»ï¼Œç”¨äºæ˜¾ç¤ºæ“ä½œè¿›åº¦
    åŸºäºtqdmåº“å®ç°ï¼Œæä¾›ç®€å•çš„è¿›åº¦æ˜¾ç¤ºç•Œé¢
    """
    def __init__(self, total, desc="è¿›åº¦"):
        """
        åˆå§‹åŒ–è¿›åº¦æ¡
        :param total: æ€»ä»»åŠ¡æ•°
        :param desc: è¿›åº¦æ¡æè¿°æ–‡å­—
        """
        self.pbar = tqdm(
            total=total,
            desc=desc,
            unit="æ¡",
            ncols=80,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]"
        )

    def update(self, n=1):
        """æ›´æ–°è¿›åº¦"""
        self.pbar.update(n)

    def close(self):
        """å…³é—­è¿›åº¦æ¡"""
        self.pbar.close()

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()

# ç»ˆç«¯é¢œè‰²è¾“å‡ºå‡½æ•°
def print_colored(text, color=Fore.WHITE, style=Style.NORMAL):
    """
    æ‰“å°å½©è‰²æ–‡æœ¬
    :param text: è¦æ‰“å°çš„æ–‡æœ¬
    :param color: æ–‡æœ¬é¢œè‰²
    :param style: æ–‡æœ¬æ ·å¼
    """
    print(f"{style}{color}{text}{Style.RESET_ALL}")

def print_success(text):
    """æ‰“å°æˆåŠŸä¿¡æ¯ï¼ˆç»¿è‰²ï¼‰"""
    print_colored(text, Fore.GREEN, Style.BRIGHT)

def print_error(text):
    """æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ˆçº¢è‰²ï¼‰"""
    print_colored(text, Fore.RED, Style.BRIGHT)

def print_warning(text):
    """æ‰“å°è­¦å‘Šä¿¡æ¯ï¼ˆé»„è‰²ï¼‰"""
    print_colored(text, Fore.YELLOW, Style.BRIGHT)

def print_info(text):
    """æ‰“å°æ™®é€šä¿¡æ¯ï¼ˆé’è‰²ï¼‰"""
    print_colored(text, Fore.CYAN, Style.NORMAL)

class RateLimitTracker:
    """
    APIé€Ÿç‡é™åˆ¶è¿½è¸ªå™¨
    ç”¨äºç®¡ç†Twitter APIçš„è¯·æ±‚é™åˆ¶ï¼Œé¿å…è¶…å‡ºé…é¢
    åŒ…å«è¯·æ±‚è®¡æ•°ã€ç­‰å¾…æ—¶é—´è®¡ç®—å’Œæ™ºèƒ½é€€é¿ç­–ç•¥
    """
    def __init__(self):
        # åˆå§‹åŒ–æ¯ä¸ªç«¯ç‚¹çš„è¯·æ±‚è®°å½•
        self.requests = {
            'get_user': [],         # è·å–ç”¨æˆ·ä¿¡æ¯çš„è¯·æ±‚è®°å½•
            'get_users_tweets': []  # è·å–ç”¨æˆ·æ¨æ–‡çš„è¯·æ±‚è®°å½•
        }
        self.window_size = RATE_LIMIT_WINDOW  # æ—¶é—´çª—å£å¤§å°
        self.endpoint_limits = {
            'get_user': {'limit': 900, 'window': 900},        # 900æ¬¡/15åˆ†é’Ÿ
            'get_users_tweets': {'limit': 180, 'window': 900} # 180æ¬¡/15åˆ†é’Ÿ
        }
        self.min_interval = 3.0     # æœ€å°è¯·æ±‚é—´éš”(ç§’)
        self.backoff_factor = 1.5   # é€€é¿å› å­

    def add_request(self, endpoint):
        """
        è®°å½•æ–°çš„APIè¯·æ±‚
        :param endpoint: APIç«¯ç‚¹åç§°
        """
        current_time = time.time()
        if endpoint not in self.requests:
            self.requests[endpoint] = []
        self.requests[endpoint].append(current_time)
        self._cleanup_old_requests(endpoint, current_time)

    def _cleanup_old_requests(self, endpoint, current_time):
        """
        æ¸…ç†è¶…å‡ºæ—¶é—´çª—å£çš„æ—§è¯·æ±‚è®°å½•
        :param endpoint: APIç«¯ç‚¹åç§°
        :param current_time: å½“å‰æ—¶é—´æˆ³
        """
        if endpoint not in self.endpoint_limits:
            return
        window = self.endpoint_limits[endpoint]['window']
        cutoff_time = current_time - window
        self.requests[endpoint] = [t for t in self.requests[endpoint] if t > cutoff_time]

    def get_remaining_requests(self, endpoint):
        """
        è·å–æŒ‡å®šç«¯ç‚¹çš„å‰©ä½™è¯·æ±‚é…é¢
        :param endpoint: APIç«¯ç‚¹åç§°
        :return: å‰©ä½™å¯ç”¨è¯·æ±‚æ•°
        """
        if endpoint not in self.endpoint_limits:
            return 999  # å¯¹äºæœªçŸ¥ç«¯ç‚¹ï¼Œè¿”å›ä¸€ä¸ªå¤§æ•°
        self._cleanup_old_requests(endpoint, time.time())
        limit = self.endpoint_limits[endpoint]['limit']
        count = len(self.requests.get(endpoint, []))
        remaining = max(0, limit - count)
        
        # å½“é…é¢è¾ƒä½æ—¶è®°å½•è­¦å‘Š
        if remaining < limit * 0.2:  # å‰©ä½™é…é¢ä½äº20%
            logger.warning(f"{endpoint} å‰©ä½™é…é¢è¾ƒä½: {remaining}/{limit}")
        
        return remaining

    def should_wait(self, endpoint):
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…åå†å‘é€è¯·æ±‚
        :param endpoint: APIç«¯ç‚¹åç§°
        :return: (æ˜¯å¦éœ€è¦ç­‰å¾…, éœ€è¦ç­‰å¾…çš„æ—¶é—´)
        """
        current_time = time.time()
        
        # ç¡®ä¿ç«¯ç‚¹å­˜åœ¨
        if endpoint not in self.requests:
            self.requests[endpoint] = []
        
        # è·å–è¯·æ±‚è®°å½•
        requests = self.requests[endpoint]
        
        # æ£€æŸ¥æœ€å°é—´éš”
        if requests:
            last_request = max(requests)
            time_since_last = current_time - last_request
            if time_since_last < self.min_interval:
                return True, self.min_interval - time_since_last

        # æ£€æŸ¥é…é¢
        remaining = self.get_remaining_requests(endpoint)
        if remaining <= 5:  # å½“å‰©ä½™é…é¢è¾ƒä½æ—¶å¢åŠ ç­‰å¾…æ—¶é—´
            wait_time = self._get_wait_time(endpoint)
            # æ ¹æ®å‰©ä½™é…é¢åŠ¨æ€è°ƒæ•´ç­‰å¾…æ—¶é—´
            if remaining <= 2:
                wait_time *= self.backoff_factor
            return True, wait_time

        # æ ¹æ®è¯·æ±‚é¢‘ç‡åŠ¨æ€è°ƒæ•´é—´éš”
        if len(requests) >= 5:  # å¦‚æœæœ‰è¶³å¤Ÿçš„è¯·æ±‚å†å²
            recent_requests = sorted(requests[-5:])  # è·å–æœ€è¿‘5ä¸ªè¯·æ±‚
            avg_interval = (recent_requests[-1] - recent_requests[0]) / 4  # è®¡ç®—å¹³å‡é—´éš”
            if avg_interval < self.min_interval:
                return True, self.min_interval * self.backoff_factor

        return False, 0

    def _get_wait_time(self, endpoint):
        """
        è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
        :param endpoint: APIç«¯ç‚¹åç§°
        :return: éœ€è¦ç­‰å¾…çš„ç§’æ•°
        """
        if endpoint not in self.requests or not self.requests[endpoint]:
            return 0
        window = self.endpoint_limits.get(endpoint, {}).get('window', self.window_size)
        oldest_request = min(self.requests[endpoint])
        wait_time = max(0, (oldest_request + window) - time.time())
        
        # æ·»åŠ åŠ¨æ€ç¼“å†²æ—¶é—´
        buffer_time = min(30, wait_time * 0.1)  # æœ€å¤šé¢å¤–ç­‰å¾…30ç§’
        return wait_time + buffer_time

class RetryableTwitterClient:
    """
    å¯é‡è¯•çš„Twitterå®¢æˆ·ç«¯
    æä¾›è‡ªåŠ¨é‡è¯•ã€é”™è¯¯å¤„ç†å’Œè´Ÿè½½å‡è¡¡åŠŸèƒ½çš„Twitter APIå®¢æˆ·ç«¯
    æ”¯æŒå¤šè´¦å·è½®æ¢å’Œæ™ºèƒ½è¯·æ±‚é™åˆ¶
    """
    def __init__(self, max_retries=3, retry_delay=2):
        """
        åˆå§‹åŒ–Twitterå®¢æˆ·ç«¯
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        :param retry_delay: é‡è¯•åŸºç¡€å»¶è¿Ÿæ—¶é—´(ç§’)
        """
        self.clients = []              # å®¢æˆ·ç«¯åˆ—è¡¨
        self.current_client_index = 0  # å½“å‰ä½¿ç”¨çš„å®¢æˆ·ç«¯ç´¢å¼•
        self.rate_limiter = RateLimitTracker()  # é€Ÿç‡é™åˆ¶è¿½è¸ªå™¨
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.initialize_all_clients()
        # é”™è¯¯è®¡æ•°å™¨
        self.error_counts = {
            'network': 0,    # ç½‘ç»œé”™è¯¯
            'auth': 0,       # è®¤è¯é”™è¯¯
            'rate_limit': 0, # é€Ÿç‡é™åˆ¶é”™è¯¯
            'other': 0       # å…¶ä»–é”™è¯¯
        }
        # ä¸åŒç±»å‹é”™è¯¯çš„é€€é¿æ—¶é—´
        self.backoff_times = {
            'network': 10,    # ç½‘ç»œé”™è¯¯ç­‰å¾…æ—¶é—´
            'auth': 15,       # è®¤è¯é”™è¯¯ç­‰å¾…æ—¶é—´
            'rate_limit': 30, # é€Ÿç‡é™åˆ¶ç­‰å¾…æ—¶é—´
            'other': 5        # å…¶ä»–é”™è¯¯ç­‰å¾…æ—¶é—´
        }

    def initialize_all_clients(self):
        """
        åˆå§‹åŒ–æ‰€æœ‰Twitter APIå®¢æˆ·ç«¯
        ä»ç¯å¢ƒå˜é‡åŠ è½½è®¤è¯ä¿¡æ¯å¹¶åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
        """
        for i in range(1, 3):  # æ”¯æŒä¸¤ä¸ªè´¦å·
            try:
                client = tweepy.Client(
                    bearer_token=os.getenv(f'TWITTER_BEARER_TOKEN_{i}'),
                    consumer_key=os.getenv(f'TWITTER_API_KEY_{i}'),
                    consumer_secret=os.getenv(f'TWITTER_API_KEY_SECRET_{i}'),
                    access_token=os.getenv(f'TWITTER_ACCESS_TOKEN_{i}'),
                    access_token_secret=os.getenv(f'TWITTER_ACCESS_TOKEN_SECRET_{i}'),
                    wait_on_rate_limit=False
                )
                self.clients.append({
                    'name': f'account{i}',
                    'client': client,
                    'is_active': True
                })
                logger.info(f"Twitter APIå®¢æˆ·ç«¯ account{i} åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"APIå®¢æˆ·ç«¯ account{i} åˆå§‹åŒ–é”™è¯¯: {str(e)}")

    def get_next_active_client(self):
        """
        è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„å®¢æˆ·ç«¯
        :return: å¯ç”¨çš„å®¢æˆ·ç«¯é…ç½®ï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨å®¢æˆ·ç«¯åˆ™è¿”å›None
        """
        original_index = self.current_client_index
        while True:
            self.current_client_index = (self.current_client_index + 1) % len(self.clients)
            if self.clients[self.current_client_index]['is_active']:
                return self.clients[self.current_client_index]
            if self.current_client_index == original_index:
                return None

    def _get_error_type(self, error):
        """
        è¯†åˆ«é”™è¯¯ç±»å‹
        :param error: æ•è·çš„å¼‚å¸¸
        :return: é”™è¯¯ç±»å‹å­—ç¬¦ä¸²
        """
        if isinstance(error, tweepy.errors.TooManyRequests):
            return 'rate_limit'
        elif isinstance(error, (ssl.SSLError, SSLError, URLLibSSLError)):
            return 'network'
        elif isinstance(error, tweepy.errors.Unauthorized):
            return 'auth'
        elif isinstance(error, tweepy.errors.TweepyException):
            if '429' in str(error):  # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                return 'rate_limit'
            return 'network'
        return 'other'

    def _get_wait_time(self, error_type, retry_count):
        """
        è®¡ç®—æ™ºèƒ½ç­‰å¾…æ—¶é—´
        :param error_type: é”™è¯¯ç±»å‹
        :param retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        :return: éœ€è¦ç­‰å¾…çš„ç§’æ•°
        """
        base_time = self.backoff_times[error_type]
        if error_type == 'rate_limit':
            # å¯¹äºé€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œä½¿ç”¨æ›´æ¿€è¿›çš„é€€é¿ç­–ç•¥
            wait_time = base_time * (3 ** retry_count)  # ä½¿ç”¨3ä½œä¸ºåŸºæ•°è€Œä¸æ˜¯2
            return min(wait_time, 900)  # æœ€å¤§ç­‰å¾…15åˆ†é’Ÿ
        else:
            # å…¶ä»–é”™è¯¯ä½¿ç”¨æ™®é€šçš„æŒ‡æ•°é€€é¿
            wait_time = base_time * (2 ** retry_count)
            return min(wait_time, 300)  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ

    def make_request(self, func, endpoint, *args, **kwargs):
        """
        æ‰§è¡ŒAPIè¯·æ±‚å¹¶å¤„ç†é”™è¯¯
        :param func: è¦æ‰§è¡Œçš„APIå‡½æ•°
        :param endpoint: APIç«¯ç‚¹åç§°
        :param args: ä¼ é€’ç»™APIå‡½æ•°çš„ä½ç½®å‚æ•°
        :param kwargs: ä¼ é€’ç»™APIå‡½æ•°çš„å…³é”®å­—å‚æ•°
        :return: APIå“åº”
        :raises: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åçš„æœ€åä¸€ä¸ªå¼‚å¸¸
        """
        retries = 0
        last_error = None
        
        while retries < self.max_retries:
            current_client = self.clients[self.current_client_index]
            if not current_client['is_active']:
                next_client = self.get_next_active_client()
                if not next_client:
                    logger.error("æ‰€æœ‰APIå®¢æˆ·ç«¯éƒ½ä¸å¯ç”¨")
                    raise Exception("æ‰€æœ‰APIå®¢æˆ·ç«¯éƒ½ä¸å¯ç”¨")
                current_client = next_client
                logger.info(f"åˆ‡æ¢åˆ°APIå®¢æˆ·ç«¯: {current_client['name']}")

            try:
                # æ£€æŸ¥é€Ÿç‡é™åˆ¶
                should_wait, wait_time = self.rate_limiter.should_wait(endpoint)
                if should_wait:
                    logger.info(f"é€Ÿç‡é™åˆ¶: ç­‰å¾… {wait_time:.1f} ç§’")
                    time.sleep(wait_time)

                # è®°å½•è¯·æ±‚
                self.rate_limiter.add_request(endpoint)
                
                # æ‰§è¡Œè¯·æ±‚
                response = func(current_client['client'])
                if response is None:
                    raise Exception("APIè¿”å›ç©ºå“åº”")
                
                # æˆåŠŸåé‡ç½®è¯¥ç±»å‹çš„é”™è¯¯è®¡æ•°
                error_type = self._get_error_type(last_error) if last_error else 'other'
                self.error_counts[error_type] = 0
                
                return response
                
            except Exception as e:
                error_type = self._get_error_type(e)
                self.error_counts[error_type] += 1
                wait_time = self._get_wait_time(error_type, retries)
                
                if error_type == 'rate_limit':
                    logger.warning(
                        f"{endpoint} é‡åˆ°é€Ÿç‡é™åˆ¶, "
                        f"ç¬¬ {retries + 1}/{self.max_retries} æ¬¡é‡è¯•, "
                        f"ç­‰å¾… {wait_time} ç§’åé‡è¯•"
                    )
                    # å½“é‡åˆ°é€Ÿç‡é™åˆ¶æ—¶ï¼Œå°†å½“å‰å®¢æˆ·ç«¯æ ‡è®°ä¸ºä¸æ´»è·ƒå¹¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
                    current_client['is_active'] = False
                    next_client = self.get_next_active_client()
                    if next_client:
                        logger.info(f"ç”±äºé€Ÿç‡é™åˆ¶ï¼Œåˆ‡æ¢åˆ°å®¢æˆ·ç«¯: {next_client['name']}")
                        continue
                else:
                    logger.warning(
                        f"{endpoint} è¯·æ±‚å¤±è´¥ ({error_type}), "
                        f"ç¬¬ {retries + 1}/{self.max_retries} æ¬¡é‡è¯•, "
                        f"ç­‰å¾… {wait_time} ç§’. é”™è¯¯: {str(e)}"
                    )
                
                # ç‰¹æ®Šé”™è¯¯å¤„ç†
                if error_type == 'auth':
                    logger.error(f"å®¢æˆ·ç«¯ {current_client['name']} è®¤è¯å¤±è´¥")
                    current_client['is_active'] = False
                    next_client = self.get_next_active_client()
                    if next_client:
                        logger.info(f"ç”±äºè®¤è¯å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å®¢æˆ·ç«¯: {next_client['name']}")
                        continue
                    break
                elif self.error_counts[error_type] >= 10:
                    logger.error(f"{error_type} é”™è¯¯æ¬¡æ•°è¿‡å¤š,è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
                    break
                
                time.sleep(wait_time)
                retries += 1
                last_error = e

        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
        if last_error:
            error_type = self._get_error_type(last_error)
            if error_type == 'rate_limit':
                logger.error(f"{endpoint} æ‰€æœ‰å®¢æˆ·ç«¯éƒ½è¾¾åˆ°APIé€Ÿç‡é™åˆ¶ï¼Œè¯·ç­‰å¾…ä¸€æ®µæ—¶é—´åå†è¯•")
            else:
                logger.error(
                    f"{endpoint} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries}), "
                    f"æœ€åä¸€æ¬¡é”™è¯¯: {str(last_error)}"
                )
            raise last_error

    def _reset_inactive_clients(self):
        """å®šæœŸæ£€æŸ¥å¹¶é‡ç½®è¢«ç¦ç”¨çš„å®¢æˆ·ç«¯"""
        current_time = time.time()
        for client in self.clients:
            if not client['is_active'] and current_time - client.get('deactivated_time', 0) > 900:
                client['is_active'] = True

def tweet_to_dict(tweet, includes=None):
    """
    å°†Tweetå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
    :param tweet: Tweetå¯¹è±¡
    :param includes: åŒ…å«åª’ä½“ã€å¼•ç”¨æ¨æ–‡ç­‰é¢å¤–ä¿¡æ¯çš„å­—å…¸
    :return: åŒ…å«æ¨æ–‡ä¿¡æ¯çš„å­—å…¸
    """
    try:
        tweet_dict = {
            'id': str(tweet.id),
            'created_at': tweet.created_at.isoformat(),
            'text': format_tweet_text(tweet),
            'metrics': {
                'like_count': tweet.public_metrics.get('like_count', 0),
                'retweet_count': tweet.public_metrics.get('retweet_count', 0),
                'reply_count': tweet.public_metrics.get('reply_count', 0),
                'quote_count': tweet.public_metrics.get('quote_count', 0)
            },
            'urls': [],
            'media': [],
            'referenced_tweets': []
        }

        # å¤„ç†URLs
        if hasattr(tweet, 'entities') and 'urls' in tweet.entities:
            for url in tweet.entities['urls']:
                url_info = {
                    'url': url.get('url'),
                    'expanded_url': url.get('expanded_url'),
                    'display_url': url.get('display_url'),
                    'title': url.get('title', ''),
                    'description': url.get('description', '')
                }
                tweet_dict['urls'].append(url_info)

        # å¤„ç†åª’ä½“é™„ä»¶
        if includes and 'media' in includes:
            media_dict = {m['media_key']: m for m in includes['media']}
            if hasattr(tweet, 'attachments') and 'media_keys' in tweet.attachments:
                for media_key in tweet.attachments['media_keys']:
                    if media_key in media_dict:
                        media = media_dict[media_key]
                        media_info = {
                            'type': media.get('type'),
                            'url': media.get('url') or media.get('preview_image_url'),
                            'height': media.get('height'),
                            'width': media.get('width'),
                            'alt_text': media.get('alt_text')
                        }
                        tweet_dict['media'].append(media_info)

        # å¤„ç†å¼•ç”¨æ¨æ–‡
        if hasattr(tweet, 'referenced_tweets') and includes and 'tweets' in includes:
            ref_tweets_dict = {t['id']: t for t in includes['tweets']}
            for ref in tweet.referenced_tweets:
                if ref['id'] in ref_tweets_dict:
                    ref_tweet = ref_tweets_dict[ref['id']]
                    ref_info = {
                        'type': ref['type'],
                        'id': ref['id'],
                        'text': ref_tweet.get('text', ''),
                        'author_id': ref_tweet.get('author_id')
                    }
                    tweet_dict['referenced_tweets'].append(ref_info)

        return tweet_dict
    except Exception as e:
        logger.error(
            f"è½¬æ¢æ¨æ–‡å¤±è´¥: {str(e)}", 
            extra={
                'tweet_id': getattr(tweet, 'id', 'unknown'),
                'error_type': type(e).__name__
            },
            exc_info=True
        )
        return None

def format_tweet_text(tweet):
    """
    æ ¼å¼åŒ–æ¨æ–‡æ–‡æœ¬ï¼Œå±•å¼€URLså¹¶æ·»åŠ åª’ä½“æè¿°
    :param tweet: Tweetå¯¹è±¡
    :return: æ ¼å¼åŒ–åçš„æ¨æ–‡æ–‡æœ¬
    """
    try:
        text = tweet.text
        if not hasattr(tweet, 'entities') or 'urls' not in tweet.entities:
            return text

        # æŒ‰URLé•¿åº¦æ’åºï¼Œé¿å…æ›¿æ¢å†²çª
        urls = sorted(tweet.entities['urls'], key=lambda x: len(x['url']), reverse=True)
        
        for url in urls:
            if 'url' in url and 'expanded_url' in url:
                # å¦‚æœæœ‰æ ‡é¢˜ï¼Œåªä½¿ç”¨æ ‡é¢˜
                if 'title' in url and url['title']:
                    replacement = url['title']
                else:
                    # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨display_url
                    replacement = url.get('display_url', url['expanded_url'])
                text = text.replace(url['url'], replacement)
        
        return text
    except Exception as e:
        logger.error(f"æ ¼å¼åŒ–æ¨æ–‡æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
        return tweet.text

class CacheManager:
    def __init__(self, cache_file=CACHE_FILE, max_size=MAX_CACHE_SIZE, expire_hours=CACHE_EXPIRE_HOURS):
        self.cache_file = cache_file
        self.max_size = max_size
        self.expire_hours = expire_hours
        self.cache_data = {}
        self.current_size = 0
        self.load_cache()

    def load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        if os.path.exists(self.cache_file):
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if os.path.getsize(self.cache_file) == 0:
                    logger.warning(f"ç¼“å­˜æ–‡ä»¶ {self.cache_file} ä¸ºç©º")
                    return

                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    try:
                        self.cache_data = json.load(f)
                        self.current_size = len(json.dumps(self.cache_data).encode('utf-8'))
                    except json.JSONDecodeError as e:
                        logger.error(f"ç¼“å­˜æ–‡ä»¶ {self.cache_file} JSONæ ¼å¼é”™è¯¯: {str(e)}")
                        self._backup_corrupted_cache()
                        self.cache_data = {}
                        self.current_size = 0

            except Exception as e:
                logger.error(f"åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
                self.cache_data = {}
                self.current_size = 0

    def _backup_corrupted_cache(self):
        """å¤‡ä»½æŸåçš„ç¼“å­˜æ–‡ä»¶"""
        try:
            backup_file = f"{self.cache_file}.bak"
            os.rename(self.cache_file, backup_file)
            logger.info(f"å·²å°†æŸåçš„ç¼“å­˜æ–‡ä»¶å¤‡ä»½ä¸º: {backup_file}")
        except Exception as e:
            logger.error(f"å¤‡ä»½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

    def save_cache(self):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        temp_file = f"{self.cache_file}.tmp"
        try:
            # æ£€æŸ¥å¹¶æ¸…ç†ç¼“å­˜å¤§å°
            self._cleanup_size()
            
            # å°†ç¼“å­˜æ•°æ®è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_cache = {}
            for key, value in self.cache_data.items():
                if 'tweets' in value:
                    serializable_cache[key] = {
                        'tweets': [self._serialize_tweet(tweet) for tweet in value['tweets']],
                        'timestamp': value['timestamp']
                    }
            
            # åŸå­å†™å…¥
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_cache, f, ensure_ascii=False, indent=2)
            os.replace(temp_file, self.cache_file)
            
            logger.info(f"ç¼“å­˜å·²ä¿å­˜åˆ°: {self.cache_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")
        finally:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except Exception as e:
                    logger.error(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _cleanup_size(self):
        """æ¸…ç†è¶…å‡ºå¤§å°é™åˆ¶çš„ç¼“å­˜æ•°æ®"""
        if self.current_size <= self.max_size:
            return

        try:
            # æŒ‰æ—¶é—´æˆ³æ’åº
            sorted_items = sorted(
                self.cache_data.items(),
                key=lambda x: datetime.fromisoformat(x[1]['timestamp'])
            )
            
            # åˆ é™¤æœ€æ—§çš„æ•°æ®ç›´åˆ°å¤§å°ç¬¦åˆè¦æ±‚
            while self.current_size > self.max_size and sorted_items:
                key, value = sorted_items.pop(0)
                value_size = len(json.dumps(value).encode('utf-8'))
                del self.cache_data[key]
                self.current_size -= value_size
            
            logger.info(f"ç¼“å­˜å¤§å°å·²æ¸…ç†è‡³ {self.current_size / 1024 / 1024:.2f}MB")
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜å¤§å°æ—¶å‡ºé”™: {str(e)}")

    def add_to_cache(self, key, value):
        """æ·»åŠ æ•°æ®åˆ°ç¼“å­˜"""
        try:
            json_str = json.dumps(value, default=str)
            value_size = len(json_str.encode('utf-8'))
            
            if self.current_size + value_size > self.max_size:
                self._cleanup_size()
            
            self.cache_data[key] = value
            self.current_size += value_size
            self.save_cache()
        except Exception as e:
            logger.error(f"æ·»åŠ ç¼“å­˜æ•°æ®å¤±è´¥: {str(e)}")

    def get(self, key):
        """è·å–ç¼“å­˜æ•°æ®"""
        return self.cache_data.get(key)

    def set(self, key, value):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        self.add_to_cache(key, value)

    def is_valid(self, key):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if key not in self.cache_data:
            return False
            
        cache_time = self.cache_data[key].get('timestamp')
        if not cache_time:
            return False
            
        try:
            cache_datetime = datetime.fromisoformat(cache_time)
            return datetime.now() - cache_datetime < timedelta(hours=self.expire_hours)
        except ValueError:
            return False

    def _serialize_tweet(self, tweet):
        """ä¸“é—¨çš„æ¨æ–‡åºåˆ—åŒ–å‡½æ•°"""
        if isinstance(tweet, dict):
            return tweet
        return {
            'id': str(tweet.id),
            'created_at': tweet.created_at.isoformat() if hasattr(tweet, 'created_at') else None,
            # ... å…¶ä»–å­—æ®µ
        }

# åˆ›å»ºå…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager()

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç±»"""
    def __init__(self):
        self.start_time = None
        self.operation = None
        self.metrics = {
            'api_calls': 0,
            'cache_hits': 0,
            'processing_time': 0
        }

    def __enter__(self):
        self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.operation:
            duration = time.time() - self.start_time
            perf_logger.info(f"{self.operation} è€—æ—¶: {duration:.2f}ç§’")

    def set_operation(self, operation):
        self.operation = operation

def log_performance(operation):
    """æ€§èƒ½æ—¥å¿—è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            with PerformanceMonitor() as monitor:
                monitor.set_operation(operation)
                return func(*args, **kwargs)
        return wrapper
    return decorator

@log_performance("è·å–ç”¨æˆ·æ¨æ–‡")
def get_user_tweets(client, username, max_results=5, include_replies=False, include_retweets=False, start_time=None, end_time=None):
    """
    è·å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
    :param client: Twitter APIå®¢æˆ·ç«¯
    :param username: ç›®æ ‡ç”¨æˆ·å
    :param max_results: è·å–çš„æ¨æ–‡æ•°é‡
    :param include_replies: æ˜¯å¦åŒ…å«å›å¤
    :param include_retweets: æ˜¯å¦åŒ…å«è½¬å‘
    :param start_time: å¼€å§‹æ—¶é—´
    :param end_time: ç»“æŸæ—¶é—´
    :return: æ¨æ–‡åˆ—è¡¨
    """
    try:
        print_info(f"\næ­£åœ¨è·å–ç”¨æˆ· {username} çš„æ¨æ–‡...")
        
        # éªŒè¯å‚æ•°
        max_results = validate_max_results(max_results)
        
        # æ„å»ºç¼“å­˜é”®
        cache_key = f"{username}_{max_results}_{include_replies}_{include_retweets}"
        if start_time:
            cache_key += f"_{start_time.isoformat()}"
        if end_time:
            cache_key += f"_{end_time.isoformat()}"
        
        # ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
        if cache_manager.is_valid(cache_key):
            logger.info("ä»ç¼“å­˜ä¸­è·å–æ•°æ®")
            return cache_manager.get(cache_key)['tweets']

        tweets_data = []
        pagination_token = None
        remaining_results = max(max_results, MIN_RESULTS_PER_REQUEST)
        retry_count = 0
        max_retries = 3

        # è·å–ç”¨æˆ·ä¿¡æ¯
        try:
            logger.info(f"æ­£åœ¨è·å–ç”¨æˆ· {username} çš„ä¿¡æ¯")
            user = client.make_request(
                lambda client: client.get_user(username=username),
                'get_user'
            )
            
            if not user or not user.data:
                logger.warning(f"æœªæ‰¾åˆ°ç”¨æˆ·: {username}")
                return []
            
            user_id = user.data.id
            logger.info(f"æ‰¾åˆ°ç”¨æˆ·ID: {user_id}")
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

        # æ„å»ºAPIå‚æ•°
        tweet_params = {
            'id': user_id,
            'max_results': min(remaining_results, MAX_RESULTS_PER_REQUEST),
            'tweet_fields': [
                'created_at',
                'text',
                'public_metrics',
                'entities',
                'referenced_tweets',
                'attachments',
                'author_id',
                'context_annotations',
                'conversation_id',
                'in_reply_to_user_id',
                'lang'
            ],
            'expansions': [
                'referenced_tweets.id',
                'referenced_tweets.id.author_id',
                'entities.mentions.username',
                'attachments.media_keys',
                'attachments.poll_ids',
                'author_id',
                'in_reply_to_user_id'
            ],
            'media_fields': [
                'url',
                'preview_image_url',
                'type',
                'height',
                'width',
                'alt_text',
                'duration_ms'
            ],
            'user_fields': [
                'name',
                'username',
                'profile_image_url',
                'verified'
            ]
        }
        
        if not include_replies:
            tweet_params['exclude'] = ['replies']
        if start_time:
            tweet_params['start_time'] = start_time.isoformat()
        if end_time:
            tweet_params['end_time'] = end_time.isoformat()

        # è·å–æ¨æ–‡
        with ProgressBar(max_results, desc="è·å–æ¨æ–‡") as pbar:
            while remaining_results > 0 and retry_count < max_retries:
                try:
                    tweet_params['max_results'] = min(remaining_results, MAX_RESULTS_PER_REQUEST)
                    if pagination_token:
                        tweet_params['pagination_token'] = pagination_token
                    
                    logger.info(f"æ­£åœ¨è·å– {tweet_params['max_results']} æ¡æ¨æ–‡")
                    tweets = client.make_request(
                        lambda client: client.get_users_tweets(**tweet_params),
                        'get_users_tweets'
                    )

                    if not tweets or not hasattr(tweets, 'data') or not tweets.data:
                        logger.warning("æœªè·å–åˆ°æ¨æ–‡æ•°æ®")
                        break

                    current_tweets = []
                    if not include_retweets:
                        current_tweets = [
                            tweet for tweet in tweets.data
                            if (not hasattr(tweet, 'referenced_tweets') or 
                                not tweet.referenced_tweets or
                                not any(ref.type == 'retweeted' for ref in tweet.referenced_tweets))
                        ]
                    else:
                        current_tweets = tweets.data

                    tweets_data.extend(current_tweets)
                    pbar.update(len(current_tweets))
                    
                    # æ›´æ–°å‰©ä½™éœ€è¦è·å–çš„æ¨æ–‡æ•°é‡
                    remaining_results = max_results - len(tweets_data)
                    
                    if remaining_results <= 0:
                        break

                    if hasattr(tweets, 'meta') and tweets.meta and tweets.meta.get('next_token') and remaining_results > 0:
                        pagination_token = tweets.meta['next_token']
                        time.sleep(2)  # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶
                    else:
                        break

                except tweepy.TooManyRequests as e:
                    wait_time = (2 ** retry_count) * 5
                    logger.warning(f"é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•")
                    time.sleep(wait_time)
                    retry_count += 1
                    continue
                    
                except Exception as e:
                    logger.error(f"è·å–æ¨æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    retry_count += 1
                    if retry_count >= max_retries:
                        break
                    time.sleep(5)

        # æ›´æ–°ç¼“å­˜
        if tweets_data:
            try:
                # å°†tweetsè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸åˆ—è¡¨
                serializable_tweets = []
                for tweet in tweets_data[:max_results]:
                    # ç¡®ä¿includesæ•°æ®å¯ç”¨
                    includes_data = tweets.includes if hasattr(tweets, 'includes') else None
                    # è½¬æ¢ä¸ºå­—å…¸
                    tweet_dict = {
                        'id': str(tweet.id),
                        'created_at': tweet.created_at.isoformat() if hasattr(tweet, 'created_at') else None,
                        'text': tweet.text if hasattr(tweet, 'text') else '',
                        'public_metrics': tweet.public_metrics if hasattr(tweet, 'public_metrics') else {},
                        'entities': tweet.entities if hasattr(tweet, 'entities') else {},
                        'referenced_tweets': tweet.referenced_tweets if hasattr(tweet, 'referenced_tweets') else [],
                        'attachments': tweet.attachments if hasattr(tweet, 'attachments') else {}
                    }
                    serializable_tweets.append(tweet_dict)

                # å­˜å…¥ç¼“å­˜
                cache_data = {
                    'tweets': serializable_tweets,
                    'timestamp': datetime.now().isoformat()
                }
                cache_manager.set(cache_key, cache_data)
                print_success(f"\næˆåŠŸè·å– {len(serializable_tweets)} æ¡æ¨æ–‡")

            except Exception as e:
                logger.error(f"å¤„ç†ç¼“å­˜æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                # å³ä½¿ç¼“å­˜å¤±è´¥ï¼Œä»ç„¶è¿”å›åŸå§‹æ•°æ®
                return tweets_data[:max_results]

        return tweets_data[:max_results]
        
    except Exception as e:
        print_error(f"\nè·å–æ¨æ–‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []
    finally:
        logger.info(f"å®Œæˆè·å–ç”¨æˆ· {username} çš„æ¨æ–‡")

@log_performance("å¯¼å‡ºæ¨æ–‡")
def export_tweets(tweets, format_type, filename=None):
    """
    å¯¼å‡ºæ¨æ–‡æ•°æ®åˆ°æ–‡ä»¶
    :param tweets: è¦å¯¼å‡ºçš„æ¨æ–‡åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯Tweetå¯¹è±¡æˆ–å­—å…¸ï¼‰
    :param format_type: å¯¼å‡ºæ ¼å¼ï¼ˆ'json' æˆ– 'csv'ï¼‰
    :param filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    :return: å¯¼å‡ºæ–‡ä»¶çš„è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    if not tweets:
        print_warning("æ²¡æœ‰å¯å¯¼å‡ºçš„æ¨æ–‡æ•°æ®")
        return None
        
    try:
        if format_type not in EXPORT_FORMATS:
            print_error(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
            return None
            
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tweets_{timestamp}.{format_type}"
            
        print_info(f"\næ­£åœ¨å¯¼å‡º {len(tweets)} æ¡æ¨æ–‡åˆ° {filename}...")
        
        with ProgressBar(len(tweets), desc="å¯¼å‡ºæ•°æ®") as pbar:
            if format_type == 'json':
                data = []
                for tweet in tweets:
                    if isinstance(tweet, dict):
                        data.append(tweet)
                    else:
                        tweet_dict = tweet_to_dict(tweet)
                        if tweet_dict:
                            data.append(tweet_dict)
                    pbar.update()
                    
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
            elif format_type == 'csv':
                import csv
                with open(filename, 'w', encoding='utf-8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(['ID', 'å‘å¸ƒæ—¶é—´', 'å†…å®¹', 'ç‚¹èµæ•°', 'è½¬å‘æ•°', 'å›å¤æ•°'])
                    
                    for tweet in tweets:
                        if isinstance(tweet, dict):
                            metrics = tweet.get('metrics', {})
                            writer.writerow([
                                tweet.get('id', ''),
                                tweet.get('created_at', ''),
                                tweet.get('text', ''),
                                metrics.get('like_count', 0),
                                metrics.get('retweet_count', 0),
                                metrics.get('reply_count', 0)
                            ])
                        else:
                            metrics = tweet.public_metrics if hasattr(tweet, 'public_metrics') else {}
                            writer.writerow([
                                tweet.id,
                                tweet.created_at.isoformat() if hasattr(tweet, 'created_at') else '',
                                format_tweet_text(tweet),
                                metrics.get('like_count', 0),
                                metrics.get('retweet_count', 0),
                                metrics.get('reply_count', 0)
                            ])
                        pbar.update()
                        
        print_success(f"\næ•°æ®å·²æˆåŠŸå¯¼å‡ºåˆ°: {filename}")
        return filename
        
    except Exception as e:
        print_error(f"\nå¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        logger.error(f"å¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None

def validate_max_results(value):
    """
    éªŒè¯è·å–æ¨æ–‡æ•°é‡å‚æ•°
    :param value: è¦éªŒè¯çš„æ•°å€¼
    :return: éªŒè¯åçš„æ•´æ•°å€¼
    :raises ValueError: å½“è¾“å…¥å€¼æ— æ•ˆæ—¶
    """
    try:
        max_results = int(value)
        if max_results <= 0:
            raise ValueError("è·å–æ¨æ–‡æ•°é‡å¿…é¡»å¤§äº0")
        return max_results
    except ValueError as e:
        raise ValueError(f"æ— æ•ˆçš„æ¨æ–‡æ•°é‡: {str(e)}")

def handle_get_tweets(twitter_client):
    """
    å¤„ç†è·å–æ¨æ–‡çš„åŠŸèƒ½
    åŒ…æ‹¬ç”¨æˆ·è¾“å…¥å¤„ç†ã€å‚æ•°éªŒè¯ã€æ¨æ–‡è·å–å’Œæ˜¾ç¤º
    :param twitter_client: Twitter APIå®¢æˆ·ç«¯
    """
    try:
        target_username = input("è¯·è¾“å…¥è¦çˆ¬å–çš„Twitterç”¨æˆ·åï¼ˆä¸åŒ…å«@ç¬¦å·ï¼‰: ").strip()
        if not target_username:
            print_error("ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
            return

        max_results = input("è¯·è¾“å…¥è¦è·å–çš„æ¨æ–‡æ•°é‡ï¼ˆé»˜è®¤5æ¡ï¼‰: ").strip()
        max_results = validate_max_results(max_results) if max_results else 5

        include_replies = input("æ˜¯å¦åŒ…å«å›å¤ï¼ˆy/nï¼Œé»˜è®¤nï¼‰: ").lower().strip() == 'y'
        include_retweets = input("æ˜¯å¦åŒ…å«è½¬å‘ï¼ˆy/nï¼Œé»˜è®¤nï¼‰: ").lower().strip() == 'y'

        start_time, end_time = get_time_range()

        tweets = get_user_tweets(
            twitter_client,
            target_username,
            max_results=max_results,
            include_replies=include_replies,
            include_retweets=include_retweets,
            start_time=start_time,
            end_time=end_time
        )

        if not tweets:
            return

        display_tweets(tweets)
        handle_export(tweets)

    except ValueError as e:
        print_error(f"è¾“å…¥é”™è¯¯: {str(e)}")
    except Exception as e:
        print_error(f"æ“ä½œå¤±è´¥: {str(e)}")
        logger.error(f"è·å–æ¨æ–‡æ“ä½œå¤±è´¥: {str(e)}", exc_info=True)

def get_time_range():
    """
    è·å–ç”¨æˆ·è¾“å…¥çš„æ—¶é—´èŒƒå›´
    :return: (å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´) çš„å…ƒç»„ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä¸ºNone
    """
    start_time = end_time = None
    use_time_filter = input("æ˜¯å¦æŒ‰æ—¶é—´èŒƒå›´ç­›é€‰ï¼ˆy/nï¼Œé»˜è®¤nï¼‰: ").lower().strip() == 'y'

    if use_time_filter:
        try:
            print("è¯·è¾“å…¥èµ·å§‹æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼Œç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶ï¼‰:")
            start_date = input().strip()
            if start_date:
                start_time = datetime.strptime(start_date, '%Y-%m-%d')

            print("è¯·è¾“å…¥ç»“æŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼Œç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶ï¼‰:")
            end_date = input().strip()
            if end_date:
                end_time = datetime.strptime(end_date, '%Y-%m-%d')
        except ValueError as e:
            print_error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}")
            return None, None

    return start_time, end_time

def display_tweets(tweets, format_type='detailed'):
    """
    æ˜¾ç¤ºæ¨æ–‡å†…å®¹
    :param tweets: æ¨æ–‡åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯Tweetå¯¹è±¡æˆ–å­—å…¸ï¼‰
    :param format_type: æ˜¾ç¤ºæ ¼å¼ï¼ˆ'detailed' æˆ– 'simple' æˆ– 'compact'ï¼‰
    """
    for tweet in tweets:
        try:
            print(f"\n{'='*80}")
            
            # åˆ¤æ–­æ˜¯Tweetå¯¹è±¡è¿˜æ˜¯å­—å…¸
            if isinstance(tweet, dict):
                # å·²ç»æ˜¯å­—å…¸æ ¼å¼
                created_at = tweet.get('created_at', 'N/A')
                text = tweet.get('text', 'N/A')
                media = tweet.get('media', [])
                referenced_tweets = tweet.get('referenced_tweets', [])
                metrics = tweet.get('metrics', {})
            else:
                # Tweetå¯¹è±¡ï¼Œéœ€è¦è½¬æ¢
                created_at = tweet.created_at.isoformat() if hasattr(tweet, 'created_at') else 'N/A'
                text = format_tweet_text(tweet)
                media = tweet.media if hasattr(tweet, 'media') else []
                referenced_tweets = tweet.referenced_tweets if hasattr(tweet, 'referenced_tweets') else []
                metrics = tweet.public_metrics if hasattr(tweet, 'public_metrics') else {}
            
            print(f"å‘å¸ƒæ—¶é—´: {created_at}")
            print(f"\n{text}\n")
            
            # æ˜¾ç¤ºåª’ä½“å†…å®¹
            if media:
                print("\nåª’ä½“å†…å®¹:")
                for m in media:
                    print(f"- ç±»å‹: {m.get('type', 'N/A')}")
                    print(f"  é“¾æ¥: {m.get('url', 'N/A')}")
                    if m.get('alt_text'):
                        print(f"  æè¿°: {m['alt_text']}")
            
            # æ˜¾ç¤ºå¼•ç”¨æ¨æ–‡
            if referenced_tweets:
                print("\nå¼•ç”¨æ¨æ–‡:")
                for ref in referenced_tweets:
                    print(f"- ç±»å‹: {ref.get('type', 'N/A')}")
                    print(f"  å†…å®¹: {ref.get('text', 'N/A')}")
            
            # æ˜¾ç¤ºäº’åŠ¨æ•°æ®
            print(f"\nğŸ‘ {metrics.get('like_count', 0)} | ğŸ”„ {metrics.get('retweet_count', 0)} | ğŸ’¬ {metrics.get('reply_count', 0)} | ğŸ“ {metrics.get('quote_count', 0)}")
            print(f"{'='*80}")
            
        except Exception as e:
            logger.error(f"æ˜¾ç¤ºæ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
            print_error("æ˜¾ç¤ºè¯¥æ¡æ¨æ–‡æ—¶å‡ºé”™ï¼Œå·²è·³è¿‡")

def handle_export(tweets):
    """
    å¤„ç†å¯¼å‡ºåŠŸèƒ½
    æä¾›äº¤äº’å¼çš„å¯¼å‡ºæ ¼å¼é€‰æ‹©å’Œæ–‡ä»¶ä¿å­˜
    :param tweets: è¦å¯¼å‡ºçš„æ¨æ–‡åˆ—è¡¨
    """
    if input("\næ˜¯å¦è¦å¯¼å‡ºæ•°æ®ï¼ˆy/nï¼‰: ").lower().strip() == 'y':
        print("\nå¯ç”¨çš„å¯¼å‡ºæ ¼å¼ï¼š")
        for i, fmt in enumerate(EXPORT_FORMATS, 1):
            print_info(f"{i}. {fmt}")

        try:
            format_choice = int(input("\nè¯·é€‰æ‹©å¯¼å‡ºæ ¼å¼ï¼ˆè¾“å…¥åºå·ï¼‰: ").strip())
            if 1 <= format_choice <= len(EXPORT_FORMATS):
                format_type = EXPORT_FORMATS[format_choice - 1]
                export_tweets(tweets, format_type)
            else:
                print_error("æ— æ•ˆçš„é€‰æ‹©")
        except ValueError:
            print_error("æ— æ•ˆçš„è¾“å…¥")

def handle_api_status(twitter_client):
    """
    å¤„ç†APIçŠ¶æ€æŸ¥è¯¢åŠŸèƒ½
    æ˜¾ç¤ºå„ä¸ªAPIç«¯ç‚¹çš„ä½¿ç”¨æƒ…å†µå’Œå‰©ä½™é…é¢
    :param twitter_client: Twitter APIå®¢æˆ·ç«¯
    """
    try:
        remaining_get_user = twitter_client.rate_limiter.get_remaining_requests('get_user')
        remaining_get_tweets = twitter_client.rate_limiter.get_remaining_requests('get_users_tweets')

        print("\nAPIä½¿ç”¨æƒ…å†µ:")
        print_info("è·å–ç”¨æˆ·ä¿¡æ¯:")
        print(f"  - æ€»é…é¢: {twitter_client.rate_limiter.endpoint_limits['get_user']['limit']}")
        print(f"  - å·²ä½¿ç”¨: {twitter_client.rate_limiter.endpoint_limits['get_user']['limit'] - remaining_get_user}")
        print(f"  - å‰©ä½™é…é¢: {remaining_get_user}")

        if twitter_client.rate_limiter.requests['get_user']:
            print(f"  - é‡ç½®æ—¶é—´: {twitter_client.rate_limiter._get_wait_time('get_user'):.1f} ç§’å")

        print_info("\nè·å–æ¨æ–‡:")
        print(f"  - æ€»é…é¢: {twitter_client.rate_limiter.endpoint_limits['get_users_tweets']['limit']}")
        print(f"  - å·²ä½¿ç”¨: {twitter_client.rate_limiter.endpoint_limits['get_users_tweets']['limit'] - remaining_get_tweets}")
        print(f"  - å‰©ä½™é…é¢: {remaining_get_tweets}")

        if twitter_client.rate_limiter.requests['get_users_tweets']:
            print(f"  - é‡ç½®æ—¶é—´: {twitter_client.rate_limiter._get_wait_time('get_users_tweets'):.1f} ç§’å")

    except Exception as e:
        print_error(f"è·å–APIä½¿ç”¨æƒ…å†µæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"è·å–APIçŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)

def handle_clear_cache():
    """
    å¤„ç†æ¸…é™¤ç¼“å­˜åŠŸèƒ½
    åˆ é™¤ç¼“å­˜æ–‡ä»¶å¹¶æ˜¾ç¤ºæ“ä½œç»“æœ
    """
    try:
        if os.path.exists(CACHE_FILE):
            os.remove(CACHE_FILE)
            print_success("ç¼“å­˜å·²æ¸…é™¤")
        else:
            print_warning("æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")
    except Exception as e:
        print_error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")
        logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)

def main():
    """
    ä¸»å‡½æ•°
    æä¾›äº¤äº’å¼èœå•ï¼Œå¤„ç†ç”¨æˆ·é€‰æ‹©å’Œç¨‹åºæµç¨‹æ§åˆ¶
    åŒ…æ‹¬ï¼š
    1. åˆå§‹åŒ–Twitterå®¢æˆ·ç«¯
    2. æä¾›åŠŸèƒ½èœå•
    3. å¤„ç†ç”¨æˆ·è¾“å…¥
    4. é”™è¯¯å¤„ç†å’Œç¨‹åºé€€å‡º
    """
    try:
        twitter_client = RetryableTwitterClient()
        if not twitter_client.clients:
            print_error("APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
            return

        actions = {
            '1': lambda: handle_get_tweets(twitter_client),
            '2': lambda: handle_api_status(twitter_client),
            '3': handle_clear_cache,
            '4': lambda: print_warning("\nè¯·æ³¨æ„ï¼šåªèƒ½å¯¼å‡ºæœ€è¿‘ä¸€æ¬¡è·å–çš„æ¨æ–‡æ•°æ®\nå¦‚éœ€å¯¼å‡ºå…¶ä»–æ•°æ®ï¼Œè¯·å…ˆä½¿ç”¨é€‰é¡¹1è·å–æ¨æ–‡")
        }

        while True:
            print("\n" + "="*20 + " Twitterçˆ¬è™«å·¥å…· " + "="*20)
            print_info("""
1. è·å–ç”¨æˆ·æ¨æ–‡
2. æ˜¾ç¤ºAPIä½¿ç”¨æƒ…å†µ
3. æ¸…é™¤ç¼“å­˜
4. å¯¼å‡ºæ•°æ®
q. é€€å‡º
            """)

            choice = input("\nè¯·é€‰æ‹©æ“ä½œ: ").strip()

            if choice == 'q':
                print_info("\næ„Ÿè°¢ä½¿ç”¨,å†è§!")
                break
            elif choice in actions:
                actions[choice]()
            else:
                print_error("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•")

    except KeyboardInterrupt:
        print_info("\n\nç¨‹åºå·²ç»ˆæ­¢")
    except Exception as e:
        print_error(f"\nç¨‹åºå‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"ç¨‹åºè¿è¡Œæ—¶é”™è¯¯: {str(e)}", exc_info=True)
    finally:
        colorama.deinit()

if __name__ == "__main__":
    main() 